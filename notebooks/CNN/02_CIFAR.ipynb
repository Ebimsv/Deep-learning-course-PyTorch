{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torchvision import datasets, models\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom CNN class\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define function for loading pre-trained model (ResNet-18)\n",
    "def load_resnet18(num_classes=10):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Helper Classes: `AverageMeter` class for tracking loss and other metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.sum += value * n\n",
    "        self.count += n\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.sum / self.count if self.count > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trasform and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced preprocessing and data augmentation\n",
    "transform_train = T.Compose([T.RandomHorizontalFlip(),\n",
    "                             T.RandomRotation(10),\n",
    "                             T.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
    "                             T.ToTensor(),\n",
    "                             T.Normalize((0.5, 0.5, 0.5), \n",
    "                                         (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform_val = T.Compose([T.ToTensor(),\n",
    "                           T.Normalize((0.5, 0.5, 0.5), \n",
    "                                       (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform_train, download=True)\n",
    "val_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_val, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model (custom CNN or ResNet-18)\n",
    "model_type = \"custom\"  # change to \"resnet\" for ResNet-18\n",
    "num_classes = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if model_type == \"custom\":\n",
    "    model = SimpleCNN(num_classes=num_classes).to(device)\n",
    "    summary(model, (3, 32, 32))\n",
    "else:\n",
    "    model = load_resnet18(num_classes=num_classes).to(device)\n",
    "    summary(model, (3, 32, 32))\n",
    "\n",
    "\n",
    "# Define criterion, optimizer, and metrics\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "val_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation functions\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch, accuracy_metric):\n",
    "    model.train()\n",
    "    loss_meter = AverageMeter()\n",
    "    accuracy_metric.reset()\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Training]\", leave=False)\n",
    "    \n",
    "    for X_batch, y_batch in progress_bar:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update loss and accuracy\n",
    "        loss_meter.update(loss.item(), X_batch.size(0))\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        accuracy_metric.update(preds, y_batch)\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss_meter.avg, accuracy=accuracy_metric.compute().item())\n",
    "        \n",
    "    avg_loss = loss_meter.avg\n",
    "    avg_accuracy = accuracy_metric.compute().item()\n",
    "    \n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, epoch, accuracy_metric):\n",
    "    model.eval()\n",
    "    loss_meter = AverageMeter()\n",
    "    accuracy_metric.reset()\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Validation]\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in progress_bar:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Update loss and accuracy\n",
    "            loss_meter.update(loss.item(), X_batch.size(0))\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            accuracy_metric.update(preds, y_batch)\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss_meter.avg)\n",
    "        \n",
    "    avg_loss = loss_meter.avg\n",
    "    avg_accuracy = accuracy_metric.compute().item()\n",
    "    \n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard  \n",
    "writer = SummaryWriter()  \n",
    "\n",
    "# Training loop  \n",
    "num_epochs = 20  \n",
    "best_val_acc = 0.0  \n",
    "for epoch in range(num_epochs):  \n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, train_accuracy)  \n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device, epoch, val_accuracy)  \n",
    "\n",
    "    # Log metrics to TensorBoard  \n",
    "    writer.add_scalar('Loss/Train', train_loss, epoch)  \n",
    "    writer.add_scalar('Accuracy/Train', train_acc, epoch)  \n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)  \n",
    "    writer.add_scalar('Accuracy/Validation', val_acc, epoch)  \n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, \"  \n",
    "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")  \n",
    "\n",
    "    # Save best model  \n",
    "    if val_acc > best_val_acc:  \n",
    "        best_val_acc = val_acc  \n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  \n",
    "        print(f\"Best model saved at epoch {epoch+1} with validation accuracy: {best_val_acc:.4f}\")  \n",
    "\n",
    "# Close TensorBoard writer  \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss and accuracy\n",
    "def plot_metrics(metric_values, title, xlabel, ylabel):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(metric_values, label=f\"{ylabel}\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Data visualization function\n",
    "def show_sample_predictions(model, dataloader, class_names, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_batch, y_batch = next(iter(dataloader))\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for i in range(8):\n",
    "            plt.subplot(2, 4, i + 1)\n",
    "            plt.imshow(np.transpose(X_batch[i].cpu().numpy(), (1, 2, 0)) * 0.5 + 0.5)  # Un-normalize for display\n",
    "            plt.title(f\"True: {class_names[y_batch[i]]}, Pred: {class_names[preds[i]]}\")\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "# Example usage: show predictions for a batch of validation data\n",
    "class_names = train_dataset.classes  # CIFAR-10 class names\n",
    "show_sample_predictions(model, val_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard \n",
    "%tensorboard --logdir runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
